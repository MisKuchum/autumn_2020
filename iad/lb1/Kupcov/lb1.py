# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WjVXiJSaihzoNlBLkSgDNdBdR3VrhyB6
"""

pip install -U keras-tuner

import pandas as pd
import numpy as np
from tensorflow.keras.datasets import boston_housing
from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from scipy.stats.stats import pearsonr

data = pd.read_csv("/content/drive/My Drive/TraIn/train.csv")
del data['Id']
data

y = data.get('SalePrice')
x = data.drop('SalePrice', axis=1)

plt.figure(figsize=(20,8))
sns.heatmap(x.isnull(),yticklabels=False,cbar=False,cmap='viridis')
plt.show()

x.drop(labels=["Alley","PoolQC","Fence","MiscFeature"], axis=1, inplace=True)

x[x.columns] = SimpleImputer(strategy="most_frequent").fit_transform(x[x.columns])
df = pd.DataFrame(x)
x = df.apply(preprocessing.LabelEncoder().fit_transform)
 
x

# функция нормализуем данные для работы
def normalize_data(data):
  mean = data.mean(axis=0)
  std = data.std(axis=0)
  data -= mean
  data /= std
  return data

  
x_norm = normalize_data(x)
x_norm

x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.3, random_state = 2)

model = Sequential()
model.add(Dense(5059, activation="relu", input_shape=(x_train.shape[1],)))
model.add(Dropout(0.3))
model.add(Dense(1264, activation="relu"))
model.add(Dense(632, activation="relu"))
model.add(Dense(158, activation="relu"))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
 
model.summary()

history = model.fit(x_train, y_train, epochs=150, batch_size=500, verbose=1, validation_split=0.3)
print(history)
history = history.history
print("[DEBUG-USER] nn finish")

def graphs(history):
    loss = history["loss"]
    val_loss = history["val_loss"]
    epochs = range(1, len(history['loss']) + 1)
    plt.plot(epochs, loss, 'r', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
 
    plt.clf()
 
    mae = history['mae']
    val_mae = history['val_mae']
    plt.plot(epochs, mae, 'r', label='Training mae')
    plt.plot(epochs, val_mae, 'b', label='Validation mae')
    plt.title('Training and validation mae')
    plt.xlabel('Epochs')
    plt.ylabel('mae')
    plt.legend()
    plt.show()
 
 
 
# рисуем все графики
graphs(history)

pred = model.predict(x_test)
predicted_x = np.reshape(pred, (pred.shape[0]))
predicted_y = np.reshape(y_test, (y_test.shape[0]))

for test_index in range(30):
  print("Predict:", predicted_x[test_index],", True:",predicted_y.values[test_index])

cc = np.corrcoef(predicted_x, predicted_y)
ccc = cc[0][1]
print(f'Correlation Coefficient: {ccc}')

fig, ax = plt.subplots()

ax.scatter(predicted_x, predicted_y)

ax.set_xlabel(r'Величина $\mathbf{X}$')
ax.set_ylabel(r'Величина $\mathbf{Y}$')

ax.set_title('Наблюдаемые значения двух величин')

plt.show()

plt.clf()
plt.figure(figsize=(20,10))
plt.plot(range(0, len(predicted_y)), predicted_x, 'b', label='x')
plt.plot(range(0, len(predicted_y)), predicted_y, 'r', label='y')
plt.legend()
plt.show()

"""+KERAS"""

def build_model(hp):
  hidden_layers = hp.Choice('hidden_layers', values=[1,2,3])
  activation_choice = hp.Choice('activation', values=['relu', 'selu', 'elu'])
  model = Sequential()
  model.add(Dense(units=hp.Int('units',min_value=256,max_value=5059,step=768),activation=activation_choice, input_shape=(x_train.shape[1], )))
  model.add(Dropout(0.3))
  for i in range(hidden_layers):
    model.add(Dense(units=hp.Int(f'layer_{i}_units_',min_value=237//(i+1), max_value=1264//(i+1),step=316//(i+1)),activation=activation_choice))
  model.add(Dense(1))  
  model.compile(optimizer='rmsprop', loss="mse", metrics=["mae"])
  return model

! rm -rf untitled_project/

def find_best_NN(x_train, y_train):
  # создаю тюнер, который сможет подобрать оптимальную архитектуру модели
  tuner = RandomSearch(build_model, objective="val_mae", max_trials=40, executions_per_trial=1,)
  print("\n\n\n")
  # начинается автоматический подбор гиперпараметров
  print('[INFO] start searching')
  tuner.search(x_train, y_train, batch_size=500, epochs=150, validation_split=0.3)
  # выбираем лучшую модель
  print("\n\n\nRESULTS SUMMARY")
  tuner.results_summary()
  print("\n\n\n")
  # получаем лучшую модель
  print("\n\n\nHERE IS THE BEST MODEL\n\n\n")
  best_params = tuner.get_best_hyperparameters()[0]
  best_model = tuner.hypermodel.build(best_params)
  best_model.summary()
  return best_model

best_model = find_best_NN(x_train, y_train)

best_history = best_model.fit(x_train, y_train, epochs=150, batch_size=500, validation_split=0.3)
best_history = best_history.history

graphs(best_history)

kpredicted_x = best_model.predict(x_test)
kpredicted_x = np.reshape(kpredicted_x, (kpredicted_x.shape[0]))

for test_index in range(30):
  print("K:", kpredicted_x[test_index], " M:", predicted_x[test_index], " True:",predicted_y.values[test_index])

KK = np.corrcoef(kpredicted_x, y_test)
KK = KK[0][1]
print(f'My model Correlation Coefficient: {ccc}')
print(f'Keras Correlation Coefficient: {KK}')

fig, ax = plt.subplots()

ax.scatter(kpredicted_x, predicted_y)

ax.set_xlabel(r'Величина $\mathbf{X}$')
ax.set_ylabel(r'Величина $\mathbf{Y}$')

ax.set_title('Наблюдаемые значения двух величин')

plt.show()